{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo sampling in a perfect model setting\n",
    "\n",
    "Here we demonstrate the statistical sampling of KPP parameters using MCMC in a perfect model setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.activate(\"..\")\n",
    "\n",
    "using PyPlot, Printf, Statistics, OceanTurb, Dao, JLD2,\n",
    "        ColumnModelOptimizationProject, ColumnModelOptimizationProject.KPPOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize the following parameters:\n",
      "propertynames(defaults) = (:CRi, :CKE, :CNL, :CÏ„, :Cstab, :Cunst, :Cb_U, :Cb_T)\n",
      "defaults = [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36]\n",
      "perturbed = [0.294718, 4.61771, 6.62473, 0.396909, 2.02005, 6.48785, 0.580094, 1.35103]\n"
     ]
    }
   ],
   "source": [
    "     model_N = 30                # Model resolution\n",
    "    model_dt = 10*minute         # Model timestep\n",
    "initial_data = 1                 # Choose initial condition for forward runs\n",
    " target_data = (4, 7, 10)        # Target samples of saved data for model-data comparison\n",
    "        case = \"unstable_strong\" # Case to run comparison\n",
    "\n",
    "# Initialize the 'data' and the 'model'\n",
    " datadir = joinpath(\"..\", \"data\", \"coarse_perfect_model_experiment\")\n",
    "filepath = joinpath(datadir, case * \".jld2\")\n",
    "\n",
    " data = ColumnData(filepath; initial=initial_data, targets=target_data)\n",
    "model = KPPOptimization.ColumnModel(data, model_dt, N=model_N)\n",
    "\n",
    "# Pick a set of parameters to optimize\n",
    "defaults = DefaultFreeParameters(BasicParameters)\n",
    "std = DefaultStdFreeParameters(0.05, typeof(defaults))\n",
    "perturbed = NormalPerturbation(std)(defaults)\n",
    "\n",
    "println(\"We will optimize the following parameters:\")\n",
    "@show propertynames(defaults)\n",
    "@show defaults perturbed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and Log Likelihoods\n",
    "\n",
    "A basic concept we need for MCMC sampling is the notion of a 'loss function', which in a sense represents an estimate of model error, or the discrepency between model output and data.\n",
    "\n",
    "Our sampling strategy uses a 'composite' loss function constructed from error estimates for each field $U$, $V$, $T$, and $S$.\n",
    "For a field $\\Phi$ and case $i$, the loss function for a given parameter vector $\\boldsymbol{C}$ is defined\n",
    "\n",
    "$$ \\mathcal{L}^i_\\Phi(\\boldsymbol{C}, \\Phi_\\mathrm{data}^i) = \\int_{t_0}^{t_1}  \\mathrm{d} t \\sqrt{\\int_{-L_z}^0 \\mathrm{d} z \\left ( \\Phi^i_\\mathrm{model} \\left ( \\boldsymbol{C} \\right ) - \\Phi^i_\\mathrm{data} \\right )^2 }$$\n",
    "\n",
    "The total loss function for a case $i$ is then constructed from\n",
    "\n",
    "$$ \\mathcal{L}^i(\\boldsymbol{C}, \\mathrm{data}^i) = \\sum_{\\Phi} \\mathcal{W}_\\Phi \\mathcal{L}^i_\\Phi(\\boldsymbol{C}, \\Phi^i_\\mathrm{data}) \\, , $$\n",
    "\n",
    "With multiple cases or sources or data (representing different physical scenarios, flux conditions, initial conditions, *et cetera*), the value of the composite loss function is, finally\n",
    "\n",
    "$$ \\mathcal{L}(\\boldsymbol{C}, \\Sigma \\, \\mathrm{data}) = \\sum_i \\mathcal{L}^i(\\boldsymbol{C}, \\mathrm{data}^i) $$\n",
    "\n",
    "where $\\mathcal{W}_\\Phi$ essentially non-dimensionalizes the error associated with the comparison of different fields such as velocity $U$ or temperature $T$.\n",
    "\n",
    "Below, we deduce the appropriate $\\mathcal{W}_\\Phi$ for our problem based on the velocity- and temperature-specific error assocaited with our initial (perfect) parameter choices.\n",
    "\n",
    "Note that the object `MarkovLink` stores `(parameter, error)` pairs for a given parameter vector and `NegativeLogLikelihood` function. \n",
    "\n",
    "# Bayesian updates and log likelihoods\n",
    "\n",
    "The outcome of MCMC sampling is a posterior probability density $\\rho_\\mathrm{post} \\left (\\boldsymbol{C} \\, | \\, \\mathrm{data} \\right )$, which we interpret to represent the 'likelihood' of a particular vector of parameters $\\boldsymbol{C}$ being 'correct', given the data.\n",
    "$\\rho_\\mathrm{post} \\left (\\boldsymbol{C} \\, | \\, \\mathrm{data} \\right )$ is defined as\n",
    "\n",
    "$$ \\rho_\\mathrm{post} \\left ( \\boldsymbol{C} \\, | \\, \\mathrm{data} \\right ) = \\exp \\left [ - \\mathcal{L} \\left ( \\boldsymbol{C} \\right ) / \\mathcal{L}_0 \\right ] \\, \\, \\rho_\\mathrm{prior} \\left (  \\boldsymbol{C} \\right ) \\, ,$$\n",
    "\n",
    "in terms of the loss function $\\mathcal{L}(\\boldsymbol{C})$, a loss function scale $\\mathcal{L}_0$ (essentially reflecting the possible dimensionality of $\\mathcal{L}$), and a the prior distribution $\\rho_\\mathrm{prior}(\\boldsymbol{C})$, which may reflect some *a priori* information we may have about the parameters.\n",
    "For example, given that KPP has been used for 20 years, we could assume *a priori* that the parameters are Gaussian distributed around the published parameters values.\n",
    "(Here we ignore the prior, essentially assuming that $\\rho_\\mathrm{prior} = \\mathrm{constant}$, or an 'uninformative prior'.)\n",
    "Thus the negative logarithm of the likelihood of $\\boldsymbol{C}$ given the data --- the 'negative log likelihood' --- is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathrm{NLL} &\\equiv -\\log \\left [ \\rho_\\mathrm{post} \\left ( \\boldsymbol{C} \\, | \\, \\mathrm{data} \\right ) \\right ] \\\\\n",
    "&= \\mathcal{L} \\left ( \\boldsymbol{C} \\right ) \\, / \\, \\underbrace{\\mathcal{L}_0}_{\\equiv \\mathrm{\"scale\"}} - \\underbrace{\\log \\left [ \\rho \\left ( \\boldsymbol{C} \\right ) \\right ]}_{\\equiv \\mathrm{\"prior\"}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To construct a `NegativeLogLikelihood` object, we need a `model`, `data`, and a loss function.\n",
    "To understand the appropriate weighting for the loss functions associated with each field $\\Phi$, we first calculate loss functions based on temperature and velocity alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain an estimate of the relative error in the temperature and velocity fields\n",
    "test_nll_temperature = NegativeLogLikelihood(model, data, temperature_loss)\n",
    "   test_nll_velocity = NegativeLogLikelihood(model, data, velocity_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `MarkovLink`, called with the signature\n",
    "\n",
    "```julia\n",
    "link = MarkovLink(nll::NegativeLogLikelihood, C::FreeParameters)\n",
    "```\n",
    "evaluates the loss function and calculated the negative log likelihood for a set of parameters.\n",
    "\n",
    "Next, we evaluate the loss function for our chosen default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_link_velocity.error = 0.019060762141441527\n",
      "test_link_temperature.error = 0.016350740652929865\n",
      "error_ratio = test_link_velocity.error / test_link_temperature.error = 1.165743041617265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.165743041617265"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error\n",
    "test_link_temperature = MarkovLink(test_nll_temperature, perturbed)\n",
    "   test_link_velocity = MarkovLink(test_nll_velocity, perturbed)\n",
    "\n",
    "@show test_link_velocity.error test_link_temperature.error\n",
    "@show error_ratio = test_link_velocity.error / test_link_temperature.error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = (1, 1, 10, 0) = (1, 1, 10, 0)\n",
      "nll.scale = first_link.error * 1.0 = 0.17369957469231553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17369957469231553"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the weighted NLL, normalizing temperature error relative to velocity error.\n",
    "@show weights = (1, 1, 10, 0)\n",
    "\n",
    "nll = NegativeLogLikelihood(model, data, weighted_fields_loss, weights=weights)\n",
    "\n",
    "# Obtain the first link in the Markov chain\n",
    "first_link = MarkovLink(nll, defaults)\n",
    "@show nll.scale = first_link.error * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specification of Markov Chain Monte Carlo parameters\n",
    "\n",
    "Next we demonstrate how to set up a Markov Chain Monte Carlo (MCMC) sampler.\n",
    "Currently, we are only able to use the 'Hastings-Metropolis' algorithm -- a simple \n",
    "algorithm based on normally-distributed random perturbations.\n",
    "\n",
    "The algorithm consists of three steps:\n",
    "\n",
    "1. Generate a set of proposal parameters by normally perturbing the current parameters.\n",
    "2. Evaluate the Negative Log Likelihood for the proposed parameters.\n",
    "3. Decide whether to 'accept' the proposed parameters as the new current parameters on the criterion:\n",
    "\n",
    "```julia\n",
    "accept(proposal, current, scale) = current.error - proposal.error > scale * log(rand(Uniform(0, 1)))\n",
    "```\n",
    "\n",
    "Note that the right side is always negative.\n",
    "Thus if the proposal error is smaller than the current error, the proposal is always accepted.\n",
    "If the proposal error is larger than the current error, the proposal is *sometimes* accepted:\n",
    "in particular, when the uniformly distributed random number is close to 0.\n",
    "The larger tha value of 'scale', more proposals are accepted.\n",
    "The code for this algorithm is:\n",
    "\n",
    "```julia\n",
    "accepted = 0\n",
    "for i = 1:nlinks\n",
    "    proposal = MarkovLink(nll, sampler.perturb(current.param))\n",
    "    current = ifelse(accept(proposal, current, nll.scale), proposal, current)\n",
    "    push!(links, proposal)\n",
    "    if current === proposal\n",
    "        accepted += 1\n",
    "        push!(path, i)\n",
    "    else\n",
    "        @inbounds push!(path, path[end])\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "That's it.\n",
    "\n",
    "The two parameters in the MCMC algorithm are thus the 'scale' of the specified Negative Log Likelihood function, and the standard deviation of the random perturbations that generate proposal parameter sets.\n",
    "\n",
    "Below we fix the standard deviation of the random perturbations at 5\\% of the initial parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain.acceptance = 0.76\n",
      "(chain[1]).param = [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36]\n",
      "(chain[end]).param = [0.068911, 8.28092, 11.3728, 0.351559, 3.87855, 6.3413, 0.369909, 2.24388]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8-element BasicParameters{Float64}:\n",
       "  0.06891101425054781\n",
       "  8.280920097109412  \n",
       " 11.372770692978943  \n",
       "  0.3515590855438135 \n",
       "  3.8785506687101323 \n",
       "  6.341298546358863  \n",
       "  0.36990940396121613\n",
       "  2.2438757682385    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a normal perturbation with standard deviation set to 5% of default values\n",
    "std = DefaultStdFreeParameters(0.05, typeof(defaults))\n",
    "bounds = BasicParameters(((0.0*p, 3.0*p) for p in defaults)...)\n",
    "sampler = MetropolisSampler(BoundedNormalPerturbation(std, bounds))\n",
    "\n",
    "ninit = 10^2\n",
    "chain = MarkovChain(ninit, first_link, nll, sampler)\n",
    "@show chain.acceptance chain[1].param chain[end].param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting samples\n",
    "\n",
    "Finally, we collect samples and save the data. \n",
    "Fortunately, `JLD2` is able to save the entire Markov chain as a single object with no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "táµ¢: 5.86 seconds. Elapsed wall time: 0.1013 minutes.\n",
      "\n",
      "First, optimal, and last links:\n",
      "(0.17369957469231553, [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36])\n",
      "(0.10735228550263193, [0.289194, 2.72196, 7.04917, 0.498073, 5.6378, 15.3149, 1.05791, 0.994302])\n",
      "(0.2768113776928492, [0.0336146, 9.81828, 18.779, 0.659273, 3.48409, 13.9781, 0.426778, 2.25608])\n",
      " \n",
      "               length | 1100\n",
      "           acceptance | 0.715454545\n",
      " initial scaled error | 1.000000000\n",
      " optimal scaled error | 0.618034245\n",
      "\n",
      "táµ¢: 5.92 seconds. Elapsed wall time: 0.2061 minutes.\n",
      "\n",
      "First, optimal, and last links:\n",
      "(0.17369957469231553, [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36])\n",
      "(0.10735228550263193, [0.289194, 2.72196, 7.04917, 0.498073, 5.6378, 15.3149, 1.05791, 0.994302])\n",
      "(1.5330236436183398, [0.560433, 4.01063, 11.8763, 0.0400355, 1.8529, 6.949, 0.231212, 1.85852])\n",
      " \n",
      "               length | 2100\n",
      "           acceptance | 0.616666667\n",
      " initial scaled error | 1.000000000\n",
      " optimal scaled error | 0.618034245\n",
      "\n",
      "táµ¢: 6.22 seconds. Elapsed wall time: 0.3100 minutes.\n",
      "\n",
      "First, optimal, and last links:\n",
      "(0.17369957469231553, [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36])\n",
      "(0.10394248196956817, [0.215663, 4.21625, 1.69106, 0.430902, 5.67079, 18.8103, 0.270985, 1.09654])\n",
      "(1.3120558732556917, [0.086873, 12.089, 12.6014, 0.0103945, 0.74188, 6.16873, 0.0903779, 0.184594])\n",
      " \n",
      "               length | 3100\n",
      "           acceptance | 0.596129032\n",
      " initial scaled error | 1.000000000\n",
      " optimal scaled error | 0.598403779\n",
      "\n",
      "táµ¢: 6.16 seconds. Elapsed wall time: 0.4128 minutes.\n",
      "\n",
      "First, optimal, and last links:\n",
      "(0.17369957469231553, [0.3, 4.32, 6.33, 0.4, 2.0, 6.4, 0.599, 1.36])\n",
      "(0.10394248196956817, [0.215663, 4.21625, 1.69106, 0.430902, 5.67079, 18.8103, 0.270985, 1.09654])\n",
      "(0.39737860848550466, [0.00528353, 9.83985, 4.81452, 0.157253, 0.598964, 3.4798, 0.0262425, 3.6213])\n",
      " \n",
      "               length | 4100\n",
      "           acceptance | 0.564146341\n",
      " initial scaled error | 1.000000000\n",
      " optimal scaled error | 0.598403779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsave = 10^3\n",
    "chainname = \"test_markov_chain\"\n",
    "chainpath = \"$chainname.jld2\"\n",
    "@save chainpath chain\n",
    "\n",
    "tstart = time()\n",
    "for i = 1:4\n",
    "    tint = @elapsed extend!(chain, dsave)\n",
    "\n",
    "    @printf(\"táµ¢: %.2f seconds. Elapsed wall time: %.4f minutes.\\n\\n\", tint, (time() - tstart)/60)\n",
    "    @printf(\"First, optimal, and last links:\\n\")\n",
    "    println((chain[1].error, chain[1].param))\n",
    "    println((optimal(chain).error, optimal(chain).param))\n",
    "    println((chain[end].error, chain[end].param))\n",
    "    println(\" \")\n",
    "\n",
    "    println(status(chain))\n",
    "\n",
    "    oldchainpath = chainname * \"_old.jld2\"\n",
    "    mv(chainpath, oldchainpath, force=true)\n",
    "    @save chainpath chain\n",
    "    rm(oldchainpath)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we show how to create a `BatchedNegativeLogLikelihood` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = (1, 1, 10 * round(Int, mean(values(ratios)) / 10), 0) = (1, 1, 20, 0)\n",
      "first_link.error = 0.2611217428337403\n",
      "chain.acceptance = 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases = (\"free_convection\", \"unstable_weak\", \"unstable_strong\", \"stable_weak\", \"stable_strong\", \"neutral\")\n",
    "\n",
    "  datadir = joinpath(\"..\", \"data\", \"perfect_model_experiment\")\n",
    "filepaths = Dict((case, joinpath(datadir, case * \".jld2\")) for case in cases)\n",
    " datasets = Dict((case, ColumnData(filepaths[case]; initial=initial_data, targets=target_data)) for case in cases)\n",
    "   models = Dict((case, KPPOptimization.ColumnModel(datasets[case], model_dt, N=model_N)) for case in cases)\n",
    "\n",
    "defaults = DefaultFreeParameters(BasicParameters)\n",
    "@show weights = (1, 1, 10, 0)\n",
    "\n",
    "# Build the batch of NLLs.\n",
    "nll = BatchedNegativeLogLikelihood(\n",
    "    [ NegativeLogLikelihood(models[case], datasets[case], weighted_fields_loss, weights=weights)\n",
    "        for case in cases ])\n",
    "\n",
    "# Obtain the first link in the Markov chain\n",
    "first_link = MarkovLink(nll, defaults)\n",
    "nll.scale = first_link.error * 1.0\n",
    "\n",
    "std = DefaultStdFreeParameters(0.05, typeof(defaults))\n",
    "bounds = BasicParameters(((0.0*p, 3.0*p) for p in defaults)...)\n",
    "sampler = MetropolisSampler(BoundedNormalPerturbation(std, bounds))\n",
    "\n",
    "chainname = \"test_batch_markov_chain\"\n",
    "chainpath = \"$chainname.jld2\"\n",
    "dsave = 10^2\n",
    "chain = MarkovChain(dsave, first_link, nll, sampler)\n",
    "@save chainpath chain\n",
    "\n",
    "tstart = time()\n",
    "for i = 1:4\n",
    "    tint = @elapsed extend!(chain, dsave)\n",
    "\n",
    "    @printf(\"táµ¢: %.2f seconds. Elapsed wall time: %.4f minutes.\\n\\n\", tint, (time() - tstart)/60)\n",
    "    @printf(\"First, optimal, and last links:\\n\")\n",
    "    println((chain[1].error, chain[1].param))\n",
    "    println((optimal(chain).error, optimal(chain).param))\n",
    "    println((chain[end].error, chain[end].param))\n",
    "    println(\" \")\n",
    "\n",
    "    println(status(chain))\n",
    "\n",
    "    oldchainpath = chainname * \"_old.jld2\"\n",
    "    mv(chainpath, oldchainpath, force=true)\n",
    "    @save chainpath chain\n",
    "    rm(oldchainpath)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
